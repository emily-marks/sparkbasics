#Spark Basics 
A spark-basics homework covers following tasks:
* Read data from Azure ADLS gen2 Storage
* Create Spark etl job to read data from storage container
* Get latitude and longitude values from OpenCage Geocoding API if some of they were null in the Storage
* Generate geohash by Latitude & Longitude using a geohash library
* Left join weather and hotels data by generated 4-characters geohash
* Deploy Spark job on Azure Kubernetes Service (AKS)
* Store enriched data in provisioned with terraform Azure ADLS gen2 storage preserving data partitioning in parquet format in “data” container

### Set up Azure infrastructure with terraform
To access an OpenCage API a key is required. It should be provided in the  configuration `terraform/test-variables.tfvars` : 
```properties
OPEN_CAGE_KEY = "<your key generated by OpenCage API>"
```
Deploy infrastructure from `terraform` folder of the project:
```
terraform init
terraform plan -out terraform.plan  -var-file test-variables.tfvars
terraform apply terraform.plan 
```
Save the OpenCage key in environment variable:
```properties
set OPEN_CAGE_KEY=(terraform output -json | jq -r '.open_cage_key.value')
```

### Build docker image

In the `/docker` folder run `mvn clean install` command. Docker image `com.bdcc/sparkbasics:1.0.0` is built.
Tag and push image to the docker repository.
```commandline
docker tag com.bdcc/sparkbasics:1.0.0 irinasharnikovaepam/sparkbasics:1.1
docker push irinasharnikovaepam/sparkbasics:1.1
```

### Set up Azure Kubernetes Cluster
To connect the Kubernetes Cluster run commands: 

```commandline
az login
az account set --subscription <subscription-id>
az aks get-credentials --name <cluster-name> --resource-group <resource-group>
```
Create Kubernetes service account
```commandline
kubectl create serviceaccount spark
kubectl create clusterrolebinding spark-role   --clusterrole=edit   --serviceaccount=default:spark   --namespace=default
```

###Launch Spark on AKS

```commandline
spark-submit 
--master k8s://https://bdccsparkbasics-94a265b6.hcp.westeurope.azmk8s.io:443 
--deploy-mode cluster 
--name sparkbasics 
--class SparkApp 
--conf spark.kubernetes.file.upload.path=abfss://m06sparkbasics@bd201staccaparkbasics.dfs.core.windows.net/ 
--conf spark.hadoop.fs.azure.account.key.bd201staccaparkbasics.dfs.core.windows.net=zu9lFMzmo8U92SeRhJIX2ULloqrokqKWfq/GAfEgl/+KVqX+2nSTvSgh/iFpD4lN0Wku5Z1Aqu2/kILV0YUjgQ== 
--packages org.apache.hadoop:hadoop-azure:3.3.1,com.microsoft.azure:azure-storage:8.6.3,org.apache.hadoop:hadoop-common:3.3.1,com.sun.jersey:jersey-json:1.19,com.opencagedata:scala-opencage-geocoder_2.12:1.1.1,org.asynchttpclient:async-http-client:2.5.2 
--conf spark.kubernetes.authenticate.driver.serviceAccountName=spark 
--conf spark.kubernetes.container.image=docker.io/irinasharnikovaepam/sparkbasics:1.1  
--conf spark.kubernetes.container.image.pullSecrets=sparkbasics-secret 
--conf spark.hadoop.fs.azure.account.auth.type.bd201stacc.dfs.core.windows.net=OAuth  
--conf spark.hadoop.fs.azure.account.oauth.provider.type.bd201stacc.dfs.core.windows.net=org.apache.hadoop.spark.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider  
--conf spark.hadoop.fs.azure.account.oauth2.client.id.bd201stacc.dfs.core.windows.net=f3905ff9-16d4-43ac-9011-842b661d556d  
--conf spark.hadoop.fs.azure.account.oauth2.client.secret.bs201stacc.dfs.core.windows.net=mAwIU~M4~xMYHi4YX_uT8qQ.ta2.LTYZxT  
--conf spark.hadoop.fs.azure.account.oauth2.client.endpoint.bd201stacc.dfs.core.windows.net=https://login.microsoftonline.com/b41b72d0-4e9f-4c26-8a69-f949f367c91d/oauth2/token  
--conf spark.hadoop.fs.azure.account.auth.type.stsparkbasics.dfs.core.windows.net=OAuth 
--conf spark.hadoop.fs.azure.account.oauth.provider.type.stsparkbasics.dfs.core.windows.net=org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider 
--conf spark.hadoop.fs.azure.account.oauth2.client.id.stsparkbasics.dfs.core.windows.net=fb9dd1c3-38c4-4e91-8e7a-3f7ed35f45e8 
--conf spark.hadoop.fs.azure.account.oauth2.client.secret.stsparkbasics.dfs.core.windows.net=u7Alx_0vAsfr-3NE22Wx807qg.ECn-SsAD 
--conf spark.hadoop.fs.azure.account.oauth2.client.endpoint.stsparkbasics.dfs.core.windows.net=https://login.microsoftonline.com/b41b72d0-4e9f-4c26-8a69-f949f367c91d/oauth2/token 
--driver-memory 512m 
--executor-memory 2g  
--conf spark.driver.cores=1  
--conf spark.executor.instances=1 
--conf spark.executor.cores=1 
local:///opt/sparkbasics-1.0.0.jar
```

сделать сервис аккаунт (дефолтный)
kubectl create serviceaccount spark
kubectl create clusterrolebinding spark-role   --clusterrole=edit   --serviceaccount=default:spark   --namespace=default


Где найти account.key для spark-submit?


```

terraform destroy -var-file test-variables.tfvars
```

* Launch Spark app in cluster mode on AKS

```
spark-submit \
    --master k8s://https://kubernetes.docker.internal:6443 \
    --deploy-mode cluster \
    --name sparkbasics \
    --conf spark.kubernetes.container.image=irinasharnikovaepam/sparkbasics:1.1  \
    --conf fs.azure.account.auth.type.${terraform output storage-account-name}.dfs.core.windows.net=OAuth  \
    --conf fs.azure.account.oauth.provider.type.${terraform output storage-account-name}.dfs.core.windows.net=org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider  \
    --conf fs.azure.account.oauth2.client.id.${terraform output storage-account-name}.dfs.core.windows.net=${terraform output oauth2-client-id}  \
    --conf fs.azure.account.oauth2.client.secret.${terraform output storage-account-name}.dfs.core.windows.net=${terraform output oauth2-client-secret}  \
    --conf fs.azure.account.oauth2.client.endpoint.${terraform output storage-account-name}.dfs.core.windows.net=${terraform output oauth2-client-endpoint}  \
    --executor-memory 1g  \
    --class SparkApp  \
    local://data_engineer_course/scala/m06_sparkbasics_jvm_azure/target/sparkbasics-1.0.0.jar
    ...
```

docker tag com.bdcc/sparkbasics:1.0.0 irinasharnikovaepam/sparkbasics:1.1
docker push irinasharnikovaepam/sparkbasics:1.1

вот так можно запустить все через докер 
docker run com.bdcc/sparkbasics:1.0.0 driver local:///opt/sparkbasics-1.0.0.jar

права на  blob contributor нужно было ассайнить на spark-basic-app

настроить кубер на правильный хост:
az login
az account set --subscription 0b0fa67e-e377-4487-845e-c5e760fc9811
az aks get-credentials --name aks-sparkbasics-westeurope --resource-group rg-sparkbasics-westeurope
сделать сервис аккаунт (дефолтный)
kubectl create serviceaccount spark
kubectl create clusterrolebinding spark-role   --clusterrole=edit   --serviceaccount=default:spark   --namespace=default


после создания второй ноды ушла проблема с cpu
spark-submit --master spark-submit --master k8s://https://bdccsparkbasics-94a265b6.hcp.westeurope.azmk8s.io:443  --deploy-mode cluster --name sparkbasics --class SparkApp --conf spark.kubernetes.file.upload.path=abfss://m06sparkbasics@bd201staccaparkbasics.dfs.core.windows.net/ --conf spark.hadoop.fs.azure.account.key.bd201staccaparkbasics.dfs.core.windows.net=$ACCOUNT_KEY --packages org.apache.hadoop:hadoop-azure:3.3.1,com.microsoft.azure:azure-storage:8.6.3,org.apache.hadoop:hadoop-common:3.3.1,com.sun.jersey:jersey-json:1.19,com.opencagedata:scala-opencage-geocoder_2.12:1.1.1,org.asynchttpclient:async-http-client:2.5.2 --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark --conf spark.kubernetes.container.image=docker.io/irinasharnikovaepam/sparkbasics:1.1  --conf spark.hadoop.fs.azure.account.auth.type.bd201stacc.dfs.core.windows.net=OAuth  --conf spark.hadoop.fs.azure.account.oauth.provider.type.bd201stacc.dfs.core.windows.net=org.apache.hadoop.spark.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider  --conf spark.hadoop.fs.azure.account.oauth2.client.id.bd201stacc.dfs.core.windows.net=$INPUT_CLIENT_ID  --conf spark.hadoop.fs.azure.account.oauth2.client.secret.bs201stacc.dfs.core.windows.net=$INPUT_CLIENT_SECRET --conf spark.hadoop.fs.azure.account.oauth2.client.endpoint.bd201stacc.dfs.core.windows.net=$INPUT_CLIENT_ENDPOINT  --conf  spark.hadoop.fs.azure.account.auth.type.stsparkbasics.dfs.core.windows.net=OAuth --conf  spark.hadoop.fs.azure.account.oauth.provider.type.stsparkbasics.dfs.core.windows.net=org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider --conf  spark.hadoop.fs.azure.account.oauth2.client.id.stsparkbasics.dfs.core.windows.net=$OUTPUT_CLIENT_ID --conf  spark.hadoop.fs.azure.account.oauth2.client.secret.stsparkbasics.dfs.core.windows.net=$OUTPUT_CLIENT_SECRET --conf  spark.hadoop.fs.azure.account.oauth2.client.endpoint.stsparkbasics.dfs.core.windows.net=$OUTPUT_CLIENT_ENDPOINT --driver-memory 512m --executor-memory 2g  --conf spark.driver.cores=1  --conf spark.executor.instances=1 --conf spark.executor.cores=1 local:///opt/sparkbasics-1.0.0.jar --deploy-mode cluster --name sparkbasics --class SparkApp --conf spark.kubernetes.file.upload.path=abfss://m06sparkbasics@bd201staccaparkbasics.dfs.core.windows.net/ --conf spark.hadoop.fs.azure.account.key.bd201staccaparkbasics.dfs.core.windows.net=zu9lFMzmo8U92SeRhJIX2ULloqrokqKWfq/GAfEgl/+KVqX+2nSTvSgh/iFpD4lN0Wku5Z1Aqu2/kILV0YUjgQ== --packages org.apache.hadoop:hadoop-azure:3.3.1,com.microsoft.azure:azure-storage:8.6.3,org.apache.hadoop:hadoop-common:3.3.1,com.sun.jersey:jersey-json:1.19,com.opencagedata:scala-opencage-geocoder_2.12:1.1.1,org.asynchttpclient:async-http-client:2.5.2 --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark --conf spark.kubernetes.container.image=docker.io/irinasharnikovaepam/sparkbasics:1.1  --conf spark.kubernetes.container.image.pullSecrets=sparkbasics-secret --conf spark.hadoop.fs.azure.account.auth.type.bd201stacc.dfs.core.windows.net=OAuth  --conf spark.hadoop.fs.azure.account.oauth.provider.type.bd201stacc.dfs.core.windows.net=org.apache.hadoop.spark.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider  --conf spark.hadoop.fs.azure.account.oauth2.client.id.bd201stacc.dfs.core.windows.net=f3905ff9-16d4-43ac-9011-842b661d556d  --conf spark.hadoop.fs.azure.account.oauth2.client.secret.bs201stacc.dfs.core.windows.net=mAwIU~M4~xMYHi4YX_uT8qQ.ta2.LTYZxT  --conf spark.hadoop.fs.azure.account.oauth2.client.endpoint.bd201stacc.dfs.core.windows.net=https://login.microsoftonline.com/b41b72d0-4e9f-4c26-8a69-f949f367c91d/oauth2/token  --conf  spark.hadoop.fs.azure.account.auth.type.stsparkbasics.dfs.core.windows.net=OAuth --conf  spark.hadoop.fs.azure.account.oauth.provider.type.stsparkbasics.dfs.core.windows.net=org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider --conf  spark.hadoop.fs.azure.account.oauth2.client.id.stsparkbasics.dfs.core.windows.net=fb9dd1c3-38c4-4e91-8e7a-3f7ed35f45e8 --conf  spark.hadoop.fs.azure.account.oauth2.client.secret.stsparkbasics.dfs.core.windows.net=u7Alx_0vAsfr-3NE22Wx807qg.ECn-SsAD --conf  spark.hadoop.fs.azure.account.oauth2.client.endpoint.stsparkbasics.dfs.core.windows.net=https://login.microsoftonline.com/b41b72d0-4e9f-4c26-8a69-f949f367c91d/oauth2/token --driver-memory 512m --executor-memory 2g  --conf spark.driver.cores=1  --conf spark.executor.instances=1 --conf spark.executor.cores=1 local:///opt/sparkbasics-1.0.0.jar
spark-submit --master spark-submit --master k8s://https://bdccsparkbasics-94a265b6.hcp.westeurope.azmk8s.io:443  --deploy-mode cluster --name sparkbasics --class SparkApp --conf spark.kubernetes.file.upload.path=abfss://m06sparkbasics@bd201staccaparkbasics.dfs.core.windows.net/ --conf spark.hadoop.fs.azure.account.key.bd201staccaparkbasics.dfs.core.windows.net=zu9lFMzmo8U92SeRhJIX2ULloqrokqKWfq/GAfEgl/+KVqX+2nSTvSgh/iFpD4lN0Wku5Z1Aqu2/kILV0YUjgQ== --packages org.apache.hadoop:hadoop-azure:3.3.1,com.microsoft.azure:azure-storage:8.6.3,org.apache.hadoop:hadoop-common:3.3.1,com.sun.jersey:jersey-json:1.19,com.opencagedata:scala-opencage-geocoder_2.12:1.1.1,org.asynchttpclient:async-http-client:2.5.2 --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark --conf spark.kubernetes.container.image=docker.io/irinasharnikovaepam/sparkbasics:1.1  --conf spark.hadoop.fs.azure.account.auth.type.bd201stacc.dfs.core.windows.net=OAuth  --conf spark.hadoop.fs.azure.account.oauth.provider.type.bd201stacc.dfs.core.windows.net=org.apache.hadoop.spark.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider  --conf spark.hadoop.fs.azure.account.oauth2.client.id.bd201stacc.dfs.core.windows.net=f3905ff9-16d4-43ac-9011-842b661d556d  --conf spark.hadoop.fs.azure.account.oauth2.client.secret.bs201stacc.dfs.core.windows.net=mAwIU~M4~xMYHi4YX_uT8qQ.ta2.LTYZxT  --conf spark.hadoop.fs.azure.account.oauth2.client.endpoint.bd201stacc.dfs.core.windows.net=https://login.microsoftonline.com/b41b72d0-4e9f-4c26-8a69-f949f367c91d/oauth2/token  --conf  spark.hadoop.fs.azure.account.auth.type.stsparkbasics.dfs.core.windows.net=OAuth --conf  spark.hadoop.fs.azure.account.oauth.provider.type.stsparkbasics.dfs.core.windows.net=org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider --conf  spark.hadoop.fs.azure.account.oauth2.client.id.stsparkbasics.dfs.core.windows.net=fb9dd1c3-38c4-4e91-8e7a-3f7ed35f45e8 --conf  spark.hadoop.fs.azure.account.oauth2.client.secret.stsparkbasics.dfs.core.windows.net=u7Alx_0vAsfr-3NE22Wx807qg.ECn-SsAD --conf  spark.hadoop.fs.azure.account.oauth2.client.endpoint.stsparkbasics.dfs.core.windows.net=https://login.microsoftonline.com/b41b72d0-4e9f-4c26-8a69-f949f367c91d/oauth2/token --driver-memory 512m --executor-memory 2g  --conf spark.driver.cores=1  --conf spark.executor.instances=1 --conf spark.executor.cores=1 local:///opt/sparkbasics-1.0.0.jar --deploy-mode cluster --name sparkbasics --class SparkApp --conf spark.kubernetes.file.upload.path=abfss://m06sparkbasics@bd201staccaparkbasics.dfs.core.windows.net/ --conf spark.hadoop.fs.azure.account.key.bd201staccaparkbasics.dfs.core.windows.net=zu9lFMzmo8U92SeRhJIX2ULloqrokqKWfq/GAfEgl/+KVqX+2nSTvSgh/iFpD4lN0Wku5Z1Aqu2/kILV0YUjgQ== --packages org.apache.hadoop:hadoop-azure:3.3.1,com.microsoft.azure:azure-storage:8.6.3,org.apache.hadoop:hadoop-common:3.3.1,com.sun.jersey:jersey-json:1.19,com.opencagedata:scala-opencage-geocoder_2.12:1.1.1,org.asynchttpclient:async-http-client:2.5.2 --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark --conf spark.kubernetes.container.image=docker.io/irinasharnikovaepam/sparkbasics:1.1  --conf spark.kubernetes.container.image.pullSecrets=sparkbasics-secret --conf spark.hadoop.fs.azure.account.auth.type.bd201stacc.dfs.core.windows.net=OAuth  --conf spark.hadoop.fs.azure.account.oauth.provider.type.bd201stacc.dfs.core.windows.net=org.apache.hadoop.spark.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider  --conf spark.hadoop.fs.azure.account.oauth2.client.id.bd201stacc.dfs.core.windows.net=f3905ff9-16d4-43ac-9011-842b661d556d  --conf spark.hadoop.fs.azure.account.oauth2.client.secret.bs201stacc.dfs.core.windows.net=mAwIU~M4~xMYHi4YX_uT8qQ.ta2.LTYZxT  --conf spark.hadoop.fs.azure.account.oauth2.client.endpoint.bd201stacc.dfs.core.windows.net=https://login.microsoftonline.com/b41b72d0-4e9f-4c26-8a69-f949f367c91d/oauth2/token  --conf  spark.hadoop.fs.azure.account.auth.type.stsparkbasics.dfs.core.windows.net=OAuth --conf  spark.hadoop.fs.azure.account.oauth.provider.type.stsparkbasics.dfs.core.windows.net=org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider --conf  spark.hadoop.fs.azure.account.oauth2.client.id.stsparkbasics.dfs.core.windows.net=fb9dd1c3-38c4-4e91-8e7a-3f7ed35f45e8 --conf  spark.hadoop.fs.azure.account.oauth2.client.secret.stsparkbasics.dfs.core.windows.net=u7Alx_0vAsfr-3NE22Wx807qg.ECn-SsAD --conf  spark.hadoop.fs.azure.account.oauth2.client.endpoint.stsparkbasics.dfs.core.windows.net=https://login.microsoftonline.com/b41b72d0-4e9f-4c26-8a69-f949f367c91d/oauth2/token --driver-memory 512m --executor-memory 2g  --conf spark.driver.cores=1  --conf spark.executor.instances=1 --conf spark.executor.cores=1 local:///opt/sparkbasics-1.0.0.jar

удаление всех none реп
docker rmi (docker images -f “dangling=true” -q) -f

spark-submit 
--master k8s://https://bdccsparkbasics-94a265b6.hcp.westeurope.azmk8s.io:443 
--deploy-mode cluster 
--name sparkbasics 
--class SparkApp 
--conf spark.kubernetes.file.upload.path=abfss://m06sparkbasics@bd201staccaparkbasics.dfs.core.windows.net/ 
--conf spark.hadoop.fs.azure.account.key.bd201staccaparkbasics.dfs.core.windows.net=zu9lFMzmo8U92SeRhJIX2ULloqrokqKWfq/GAfEgl/+KVqX+2nSTvSgh/iFpD4lN0Wku5Z1Aqu2/kILV0YUjgQ== 
--packages org.apache.hadoop:hadoop-azure:3.3.1,com.microsoft.azure:azure-storage:8.6.3,org.apache.hadoop:hadoop-common:3.3.1,com.sun.jersey:jersey-json:1.19,com.opencagedata:scala-opencage-geocoder_2.12:1.1.1,org.asynchttpclient:async-http-client:2.5.2 
--conf spark.kubernetes.authenticate.driver.serviceAccountName=spark 
--conf spark.kubernetes.container.image=docker.io/irinasharnikovaepam/sparkbasics:1.1  
--conf spark.kubernetes.container.image.pullSecrets=sparkbasics-secret 
--conf spark.hadoop.fs.azure.account.auth.type.bd201stacc.dfs.core.windows.net=OAuth  
--conf spark.hadoop.fs.azure.account.oauth.provider.type.bd201stacc.dfs.core.windows.net=org.apache.hadoop.spark.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider  
--conf spark.hadoop.fs.azure.account.oauth2.client.id.bd201stacc.dfs.core.windows.net=f3905ff9-16d4-43ac-9011-842b661d556d  
--conf spark.hadoop.fs.azure.account.oauth2.client.secret.bs201stacc.dfs.core.windows.net=mAwIU~M4~xMYHi4YX_uT8qQ.ta2.LTYZxT  
--conf spark.hadoop.fs.azure.account.oauth2.client.endpoint.bd201stacc.dfs.core.windows.net=https://login.microsoftonline.com/b41b72d0-4e9f-4c26-8a69-f949f367c91d/oauth2/token  
--conf spark.hadoop.fs.azure.account.auth.type.stsparkbasics.dfs.core.windows.net=OAuth 
--conf spark.hadoop.fs.azure.account.oauth.provider.type.stsparkbasics.dfs.core.windows.net=org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider 
--conf spark.hadoop.fs.azure.account.oauth2.client.id.stsparkbasics.dfs.core.windows.net=fb9dd1c3-38c4-4e91-8e7a-3f7ed35f45e8 
--conf spark.hadoop.fs.azure.account.oauth2.client.secret.stsparkbasics.dfs.core.windows.net=u7Alx_0vAsfr-3NE22Wx807qg.ECn-SsAD 
--conf spark.hadoop.fs.azure.account.oauth2.client.endpoint.stsparkbasics.dfs.core.windows.net=https://login.microsoftonline.com/b41b72d0-4e9f-4c26-8a69-f949f367c91d/oauth2/token 
--driver-memory 512m 
--executor-memory 2g  
--conf spark.driver.cores=1  
--conf spark.executor.instances=1 
--conf spark.executor.cores=1 
local:///opt/sparkbasics-1.0.0.jar

spark-submit --master k8s://https://bdccsparkbasics-94a265b6.hcp.westeurope.azmk8s.io:443  --deploy-mode cluster --name sparkbasics --class SparkApp --conf spark.kubernetes.file.upload.path=abfss://m06sparkbasics@bd201staccaparkbasics.dfs.core.windows.net/ --conf spark.hadoop.fs.azure.account.key.bd201staccaparkbasics.dfs.core.windows.net=(terraform output -json | jq -r '.account_key.value') --packages org.apache.hadoop:hadoop-azure:3.3.1,com.microsoft.azure:azure-storage:8.6.3,org.apache.hadoop:hadoop-common:3.3.1,com.sun.jersey:jersey-json:1.19,com.opencagedata:scala-opencage-geocoder_2.12:1.1.1,org.asynchttpclient:async-http-client:2.5.2 --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark --conf spark.kubernetes.container.image=docker.io/irinasharnikovaepam/sparkbasics:1.1  --conf spark.kubernetes.container.image.pullSecrets=sparkbasics-secret --conf spark.hadoop.fs.azure.account.auth.type.bd201stacc.dfs.core.windows.net=OAuth  --conf spark.hadoop.fs.azure.account.oauth.provider.type.bd201stacc.dfs.core.windows.net=org.apache.hadoop.spark.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider  --conf spark.hadoop.fs.azure.account.oauth2.client.id.bd201stacc.dfs.core.windows.net=f3905ff9-16d4-43ac-9011-842b661d556d  --conf spark.hadoop.fs.azure.account.oauth2.client.secret.bs201stacc.dfs.core.windows.net=mAwIU~M4~xMYHi4YX_uT8qQ.ta2.LTYZxT  --conf spark.hadoop.fs.azure.account.oauth2.client.endpoint.bd201stacc.dfs.core.windows.net=https://login.microsoftonline.com/b41b72d0-4e9f-4c26-8a69-f949f367c91d/oauth2/token  --conf  spark.hadoop.fs.azure.account.auth.type.stsparkbasics.dfs.core.windows.net=OAuth --conf  spark.hadoop.fs.azure.account.oauth.provider.type.stsparkbasics.dfs.core.windows.net=org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider --conf  spark.hadoop.fs.azure.account.oauth2.client.id.stsparkbasics.dfs.core.windows.net=fb9dd1c3-38c4-4e91-8e7a-3f7ed35f45e8 --conf  spark.hadoop.fs.azure.account.oauth2.client.secret.stsparkbasics.dfs.core.windows.net=u7Alx_0vAsfr-3NE22Wx807qg.ECn-SsAD --conf  spark.hadoop.fs.azure.account.oauth2.client.endpoint.stsparkbasics.dfs.core.windows.net=https://login.microsoftonline.com/b41b72d0-4e9f-4c26-8a69-f949f367c91d/oauth2/token --driver-memory 512m --executor-memory 2g  --conf spark.driver.cores=1  --conf spark.executor.instances=1 --conf spark.executor.cores=1 local:///opt/sparkbasics-1.0.0.jar

terraform output -json | jq -r '.account_key.value'

set ACCOUNT_KEY=(terraform output -json | jq -r '.account_key.value')
set OPEN_CAGE_KEY=(terraform output -json | jq -r '.open_cage_key.value')
set INPUT_CLIENT_ID=(terraform output -json | jq -r '.input_client_id.value')
set INPUT_CLIENT_SECRET=(terraform output -json | jq -r '.input_client_secret.value')
set INPUT_CLIENT_ENDPOINT=(terraform output -json | jq -r '.input_client_endpoint.value')
set OUTPUT_CLIENT_ID=(terraform output -json | jq -r '.output_client_id.value')
set OUTPUT_CLIENT_SECRET=(terraform output -json | jq -r '.output_client_secret.value')
set OUTPUT_CLIENT_ENDPOINT=(terraform output -json | jq -r '.output_client_endpoint.value')
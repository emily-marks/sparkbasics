#Spark Basics 
A spark-basics homework covers following tasks:
* Read data from Azure ADLS gen2 Storage
* Create Spark etl job to read data from storage container
* Get latitude and longitude values from OpenCage Geocoding API if some of they were null in the Storage
* Generate geohash by Latitude & Longitude using a geohash library
* Left join weather and hotels data by generated 4-characters geohash
* Deploy Spark job on Azure Kubernetes Service (AKS)
* Store enriched data in provisioned with terraform Azure ADLS gen2 storage preserving data partitioning in parquet format in “data” container

### Set up Azure infrastructure with terraform
To access an OpenCage API a key is required. It should be provided in the  configuration `terraform/test-variables.tfvars` : 
```properties
OPEN_CAGE_KEY = "<your key generated by OpenCage API>"
```
Deploy infrastructure from `terraform` folder of the project:
```
terraform init
terraform plan -out terraform.plan  -var-file test-variables.tfvars
terraform apply terraform.plan 
```
Save the OpenCage key in environment variable:
```properties
set OPEN_CAGE_KEY=(terraform output -json | jq -r '.open_cage_key.value')
```

### Build docker image

In the `/docker` folder run `mvn clean install` command. Docker image `com.bdcc/sparkbasics:1.0.0` is built.
Tag and push image to the docker repository.
```commandline
docker tag com.bdcc/sparkbasics:1.0.0 irinasharnikovaepam/sparkbasics:1.1
docker push irinasharnikovaepam/sparkbasics:1.1
```

### Set up Azure Kubernetes Cluster
To connect the Kubernetes Cluster run commands: 

```commandline
az login
az account set --subscription <subscription-id>
az aks get-credentials --name <cluster-name> --resource-group <resource-group>
```
Create Kubernetes service account
```commandline
kubectl create serviceaccount spark
kubectl create clusterrolebinding spark-role   --clusterrole=edit   --serviceaccount=default:spark   --namespace=default
```

###Launch Spark on AKS

```commandline
spark-submit 
--master k8s://<cluster-url>
--deploy-mode cluster 
--name sparkbasics 
--class SparkApp 
--conf spark.kubernetes.file.upload.path=abfss://m06sparkbasics@bd201stacc.dfs.core.windows.net/  
--conf spark.kubernetes.authenticate.driver.serviceAccountName=spark 
--conf spark.kubernetes.container.image=docker.io/irinasharnikovaepam/sparkbasics:1.1  
--conf spark.hadoop.fs.azure.account.auth.type.bd201stacc.dfs.core.windows.net=OAuth  
--conf spark.hadoop.fs.azure.account.oauth.provider.type.bd201stacc.dfs.core.windows.net=org.apache.hadoop.spark.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider  
--conf spark.hadoop.fs.azure.account.oauth2.client.id.bd201stacc.dfs.core.windows.net=<client-id>  
--conf spark.hadoop.fs.azure.account.oauth2.client.secret.bs201stacc.dfs.core.windows.net=<client-secret> 
--conf spark.hadoop.fs.azure.account.oauth2.client.endpoint.bd201stacc.dfs.core.windows.net=https://login.microsoftonline.com/<directory-tenant-id>/oauth2/token  
--conf spark.hadoop.fs.azure.account.auth.type.stsparkbasics.dfs.core.windows.net=OAuth 
--conf spark.hadoop.fs.azure.account.oauth.provider.type.stsparkbasics.dfs.core.windows.net=org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider 
--conf spark.hadoop.fs.azure.account.oauth2.client.id.stsparkbasics.dfs.core.windows.net=<client-id>
--conf spark.hadoop.fs.azure.account.oauth2.client.secret.stsparkbasics.dfs.core.windows.net=<client-secret> 
--conf spark.hadoop.fs.azure.account.oauth2.client.endpoint.stsparkbasics.dfs.core.windows.net=https://login.microsoftonline.com/<directory-tenant-id>/oauth2/token 
--driver-memory 512m 
--executor-memory 2g  
--conf spark.driver.cores=1  
--conf spark.executor.instances=1 
--conf spark.executor.cores=1 
local:///opt/sparkbasics-1.0.0.jar
```
* Get `<cluster-url>` with `kubectl cluster-info`
* Use `<client-id>`, `<client-secret>`, `<directory-tenant-id>` for corresponding Azure Storage
###Destroy all resources
After completing a Spark job delete all resources
```
terraform destroy -var-file test-variables.tfvars
kubectl delete --all pods
```
